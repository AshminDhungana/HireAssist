# HireAssist - AI-Powered Resume Screening and Parsing System

## Overview

This system is a **comprehensive, production-ready Resume/CV Screening and Parsing System** built with modern technologies including **React**, **Tailwind CSS**, **Python FastAPI**, **RAG (Retrieval-Augmented Generation)**, and **LangChain**. The system leverages advanced AI techniques to automate resume screening, candidate ranking, and intelligent job matching.

**Status**: âœ… Tasks 14 & 15 Complete - Dashboard & CI/CD Live

## ğŸš€ Key Features

### Core Functionality
- âœ… **AI-Powered Resume Parsing**: Extract structured data from PDF/DOCX resumes using spaCy NLP and LangChain
- âœ… **Dual Parser System**: NLP-based (Parser A) and Regex-based (Parser B) parsers for flexible extraction
- âœ… **Parser Performance Dashboard**: Real-time comparison of parsing accuracy and speed
- âœ… **Interactive Dashboard**: Modern React interface with real-time updates
- âœ… **API Status Monitoring**: Real-time health check indicator
- **Semantic Job Matching**: RAG-powered similarity search between resumes and job descriptions
- **Real-time Candidate Ranking**: Score and rank candidates using vector embeddings
- **Intelligent Screening**: Multi-criteria evaluation with customizable scoring algorithms

### Advanced AI Features
- **Vector Database Integration**: Pinecone/Qdrant for scalable semantic search
- **Hybrid Search**: Combines keyword-based and semantic matching
- **Entity Recognition**: Extract skills, experience, education using spaCy NER
- **Adaptive Retrieval**: RAG Fusion for complex job requirement queries
- **Conversation AI**: Chat interface for querying candidate database

### Enterprise Features
- **Role-Based Authentication**: JWT-based security with Admin/Recruiter/Candidate roles
- **Multi-tenant Architecture**: Support for multiple organizations
- **API-First Design**: RESTful APIs with comprehensive documentation
- **Scalable Infrastructure**: Docker containerization and cloud deployment ready
- **Analytics Dashboard**: Comprehensive recruitment metrics and insights
- **CI/CD Pipeline**: GitHub Actions automated testing and deployment

## ğŸ›  Tech Stack

### Frontend
- **React 18+** with TypeScript/JSX
- **Tailwind CSS** for responsive design
- **Vite** for fast build tooling
- **Chart.js** for analytics visualization
- **React Query** for data fetching
- **Lucide Icons** for UI components

### Backend
- **FastAPI** with Python 3.11+
- **PostgreSQL** for primary database
- **SQLAlchemy** ORM with Alembic migrations
- **Pydantic** for data validation
- **Uvicorn** ASGI server

### AI/ML Stack
- **spaCy** for NER and text processing
- **LangChain** for RAG implementation
- **OpenAI GPT-4** for text analysis (optional)
- **Pinecone/Qdrant** vector database
- **Sentence Transformers** for embeddings
- **FAISS** for local vector search

### DevOps & Infrastructure
- **Docker** containerization
- **Docker Compose** for development
- **GitHub Actions** CI/CD
- **Nginx** reverse proxy
- **AWS/GCP** cloud deployment ready
- **Prometheus/Grafana** monitoring (planned)

## ğŸ“ Project Structure

```
HireAssist/
â”œâ”€â”€ frontend/                          # React Application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ resumeService.ts      # API service functions
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ResumeUpload.jsx      # Resume upload component
â”‚   â”‚   â”‚   â”œâ”€â”€ ApiStatus.tsx         # API health indicator
â”‚   â”‚   â”‚   â””â”€â”€ ParserComparison/     # Parser comparison dashboard
â”‚   â”‚   â”‚       â”œâ”€â”€ ComparisonCard.tsx
â”‚   â”‚   â”‚       â”œâ”€â”€ PerformanceChart.tsx
â”‚   â”‚   â”‚       â”œâ”€â”€ ComparisonTable.tsx
â”‚   â”‚   â”‚       â”œâ”€â”€ ParserComparisonDashboard.tsx
â”‚   â”‚   â”‚       â””â”€â”€ index.ts
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â””â”€â”€ ParserComparisonPage.tsx
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â”œâ”€â”€ useParserComparison.ts
â”‚   â”‚   â”‚   â””â”€â”€ useApiStatus.ts
â”‚   â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â”‚   â””â”€â”€ parser.ts
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â”œâ”€â”€ index.css
â”‚   â”‚   â””â”€â”€ main.tsx
â”‚   â”œâ”€â”€ __tests__/
â”‚   â”‚   â””â”€â”€ App.test.tsx
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â”œâ”€â”€ vitest.config.ts
â”‚   â””â”€â”€ vite.config.ts
â”œâ”€â”€ backend/                           # FastAPI Application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                   # Entry point
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ resumes.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ jobs.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ analytics.py
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”‚   â”œâ”€â”€ security.py
â”‚   â”‚   â”‚   â””â”€â”€ database.py
â”‚   â”‚   â”œâ”€â”€ models/                  # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ schemas/                 # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ services/                # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ resume_parser.py
â”‚   â”‚   â”‚   â”œâ”€â”€ parser_nlp.py        # NLP parser (Parser A)
â”‚   â”‚   â”‚   â”œâ”€â”€ parser_regex.py      # Regex parser (Parser B)
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_engine.py
â”‚   â”‚   â”‚   â”œâ”€â”€ matching_service.py
â”‚   â”‚   â”‚   â””â”€â”€ ai_service.py
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ dependencies.py
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ test_health.py
â”‚   â”œâ”€â”€ alembic/                     # Database migrations
â”‚   â”œâ”€â”€ requirements.txt              # Cleaned dependencies
â”‚   â”œâ”€â”€ .env.example
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ ai-services/                      # AI/ML microservices
â”‚   â”œâ”€â”€ resume-parser/
â”‚   â”œâ”€â”€ rag-engine/
â”‚   â””â”€â”€ vector-db/
â”œâ”€â”€ infrastructure/                   # Infrastructure as Code
â”‚   â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ kubernetes/
â”‚   â””â”€â”€ terraform/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml                   # GitHub Actions CI/CD
â”œâ”€â”€ docs/                            # Documentation
â”œâ”€â”€ scripts/                         # Deployment scripts
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docker-compose.prod.yml
â””â”€â”€ README.md
```

## ğŸ—„ Database Schema

### Core Tables
```sql
-- Users and Authentication
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    role VARCHAR(50) NOT NULL, -- admin, recruiter, candidate
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Organizations (Multi-tenant)
CREATE TABLE organizations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    domain VARCHAR(100),
    settings JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Job Postings
CREATE TABLE jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    organization_id UUID REFERENCES organizations(id),
    title VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    requirements TEXT,
    location VARCHAR(255),
    salary_range INT4RANGE,
    status VARCHAR(50) DEFAULT 'active',
    embedding VECTOR(1536), -- OpenAI embedding
    created_by UUID REFERENCES users(id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Candidates and Resumes
CREATE TABLE candidates (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    name VARCHAR(255) NOT NULL,
    email VARCHAR(255) NOT NULL,
    phone VARCHAR(50),
    location VARCHAR(255),
    summary TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE resumes (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    candidate_id UUID REFERENCES candidates(id),
    filename VARCHAR(255) NOT NULL,
    file_path VARCHAR(500) NOT NULL,
    parsed_data JSONB, -- Structured resume data
    raw_text TEXT,
    embedding VECTOR(1536),
    skills TEXT[],
    experience_years INTEGER,
    education_level VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Job Applications and Screening Results
CREATE TABLE applications (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_id UUID REFERENCES jobs(id),
    candidate_id UUID REFERENCES candidates(id),
    resume_id UUID REFERENCES resumes(id),
    status VARCHAR(50) DEFAULT 'submitted',
    applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE screening_results (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    application_id UUID REFERENCES applications(id),
    overall_score DECIMAL(5,2),
    skill_match_score DECIMAL(5,2),
    experience_score DECIMAL(5,2),
    education_score DECIMAL(5,2),
    detailed_analysis JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Note:**  
> This schema requires PostgreSQL with the `pgvector` extension enabled for embedding/vector operations.  
> Initialize your database and apply Alembic migrations to create these tables automatically.

## ğŸ¤– AI Architecture

### RAG Pipeline Architecture
```
1. Document Ingestion
   â”œâ”€â”€ Resume Upload (PDF/DOCX)
   â”œâ”€â”€ Text Extraction (PyPDF2, python-docx)
   â””â”€â”€ Text Preprocessing

2. Information Extraction
   â”œâ”€â”€ Named Entity Recognition (spaCy)
   â”œâ”€â”€ Skill Extraction (Custom NER Model)
   â””â”€â”€ Structured Data Generation

3. Vector Embedding
   â”œâ”€â”€ Text Chunking (LangChain)
   â”œâ”€â”€ Embedding Generation (OpenAI/Sentence-BERT)
   â””â”€â”€ Vector Storage (Pinecone/Qdrant)

4. Retrieval & Ranking
   â”œâ”€â”€ Semantic Search
   â”œâ”€â”€ Hybrid Search (Dense + Sparse)
   â”œâ”€â”€ Re-ranking Algorithm
   â””â”€â”€ Score Calculation

5. Generation & Response
   â”œâ”€â”€ Context Augmentation
   â”œâ”€â”€ LLM-based Analysis
   â””â”€â”€ Structured Output
```

### Matching Algorithm
```python
class CandidateMatchingService:
    def calculate_match_score(self, resume_data, job_requirements):
        """
        Multi-dimensional matching algorithm
        """
        scores = {
            'skill_match': self._calculate_skill_similarity(resume_data, job_requirements),
            'experience_match': self._calculate_experience_score(resume_data, job_requirements),
            'education_match': self._calculate_education_score(resume_data, job_requirements),
            'semantic_similarity': self._calculate_semantic_similarity(resume_data, job_requirements)
        }
        
        # Weighted scoring
        weights = {'skill_match': 0.4, 'experience_match': 0.3, 
                  'education_match': 0.2, 'semantic_similarity': 0.1}
        
        overall_score = sum(scores[key] * weights[key] for key in scores)
        return overall_score, scores
```

## ğŸ” Authentication & Security

### JWT Implementation
```python
# Security configuration
SECRET_KEY = os.getenv("SECRET_KEY")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30
REFRESH_TOKEN_EXPIRE_DAYS = 30

# Role-based access control
class RoleChecker:
    def __init__(self, allowed_roles: List[str]):
        self.allowed_roles = allowed_roles
    
    def __call__(self, current_user: User = Depends(get_current_user)):
        if current_user.role not in self.allowed_roles:
            raise HTTPException(status_code=403, detail="Insufficient permissions")
        return current_user

# Usage in routes
@router.post("/jobs", dependencies=[Depends(RoleChecker(["admin", "recruiter"]))])
async def create_job(job_data: JobCreate, current_user: User):
    # Job creation logic
    pass
```

## ğŸš€ Deployment Guide

### Development Setup
```bash
# Clone repository
git clone https://github.com/AshminDhungana/HireAssist.git
cd HireAssist

# Backend setup
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# Download spaCy model
python -m spacy download en_core_web_sm

# Environment variables
cp .env.example .env
# Edit .env with your configuration

# Start backend
uvicorn app.main:app --reload

# Frontend setup (new terminal)
cd frontend
npm install
npm run dev

# Start with Docker Compose
docker-compose up -d
```

### Production Deployment
```bash
# Build and deploy
docker-compose -f docker-compose.prod.yml up -d

# With Kubernetes
kubectl apply -f infrastructure/kubernetes/
```

### Environment Variables
```env
# Database
DATABASE_URL=postgresql://user:password@localhost:5432/resume_screening
REDIS_URL=redis://localhost:6379

# AI Services
OPENAI_API_KEY=your_openai_api_key
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=us-west1-gcp

# Authentication
SECRET_KEY=your_secret_key_here
ACCESS_TOKEN_EXPIRE_MINUTES=30

# File Storage
UPLOAD_FOLDER=./uploads
MAX_FILE_SIZE=10485760  # 10MB

# Email (for notifications)
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=your_email@gmail.com
SMTP_PASSWORD=your_app_password
```

## ğŸ“Š API Documentation

### Health Check
```
GET /api/v1/health
Response: { "status": "healthy", "message": "API is running" }
```

### Authentication Endpoints
```
POST /api/v1/auth/login      # User login
POST /api/v1/auth/register   # User registration
POST /api/v1/auth/refresh    # Refresh token
POST /api/v1/auth/logout     # Logout user
```

### Resume Management
```
POST /api/v1/resumes/upload     # Upload resume
GET  /api/v1/resumes/{id}       # Get resume details
PUT  /api/v1/resumes/{id}       # Update resume
DELETE /api/v1/resumes/{id}     # Delete resume
GET  /api/v1/resumes/parse/{id} # Parse resume content
```

### Job Management
```
POST /api/v1/jobs           # Create job posting
GET  /api/v1/jobs           # List jobs
GET  /api/v1/jobs/{id}      # Get job details
PUT  /api/v1/jobs/{id}      # Update job
DELETE /api/v1/jobs/{id}    # Delete job
```

### Screening & Matching
```
POST /api/v1/screening/match     # Match resumes to job
GET  /api/v1/screening/results   # Get screening results
POST /api/v1/screening/bulk      # Bulk screening
GET  /api/v1/screening/analytics # Screening analytics
```

## ğŸ§ª Testing Strategy

### Backend Testing
```python
# Test structure
tests/
â”œâ”€â”€ unit/              # Unit tests
â”œâ”€â”€ integration/       # Integration tests
â”œâ”€â”€ fixtures/         # Test data fixtures
â””â”€â”€ conftest.py       # Pytest configuration

# Example test
@pytest.mark.asyncio
async def test_resume_parsing():
    async with AsyncClient(app=app, base_url="http://test") as client:
        response = await client.post(
            "/api/v1/resumes/upload",
            files={"file": ("test_resume.pdf", file_content, "application/pdf")}
        )
        assert response.status_code == 201
        assert "parsed_data" in response.json()
```

### Frontend Testing
```typescript
// Component testing with React Testing Library
import { render, screen } from '@testing-library/react';
import { ResumeUpload } from './ResumeUpload';

test('renders resume upload component', () => {
  render(<ResumeUpload />);
  expect(screen.getByText(/upload resume/i)).toBeInTheDocument();
});
```

## ğŸ”§ Performance Optimization

### Caching Strategy
```python
# Redis caching
@cache(expire=3600)  # 1 hour cache
async def get_job_matches(job_id: str, limit: int = 50):
    # Expensive matching operation
    return await matching_service.find_candidates(job_id, limit)
```

### Database Optimization
```sql
-- Indexes for performance
CREATE INDEX idx_resumes_skills ON resumes USING GIN(skills);
CREATE INDEX idx_resumes_embedding ON resumes USING ivfflat (embedding vector_cosine_ops);
CREATE INDEX idx_jobs_created_at ON jobs(created_at);
```

## ğŸ“ˆ Monitoring & Analytics

### Metrics Collection
```python
# Prometheus metrics
from prometheus_client import Counter, Histogram

resume_uploads = Counter('resume_uploads_total', 'Total resume uploads')
screening_duration = Histogram('screening_duration_seconds', 'Time spent screening')

@router.post("/resumes/upload")
async def upload_resume():
    resume_uploads.inc()
    with screening_duration.time():
        # Processing logic
        pass
```

### Health Checks
```python
@router.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "database": await check_database_connection(),
        "redis": await check_redis_connection(),
        "vector_db": await check_vector_db_connection()
    }
```

## âœ¨ Current Implementation Status

### âœ… Completed (Tasks 14 & 15)

#### Task 14: React Dashboard
- âœ… Parser comparison page with metrics
- âœ… Side-by-side comparison cards (Parser A vs B)
- âœ… Performance visualization with Chart.js
- âœ… Detailed comparison table
- âœ… Professional Tailwind CSS styling
- âœ… Responsive mobile design
- âœ… API status indicator badge
- âœ… Full TypeScript type safety

#### Task 15: CI/CD Pipeline
- âœ… GitHub Actions workflow (`.github/workflows/ci.yml`)
- âœ… Backend automated testing (pytest)
- âœ… Frontend build verification (Vite)
- âœ… Code quality checks
- âœ… Runs on push to main branch
- âœ… Blocks merge if tests fail

### ğŸ“‹ In Progress
- Resume parsing API endpoints
- File upload handling
- Database integration

### ğŸ”œ Planned
- Advanced job matching algorithm
- Vector database integration
- Multi-tenant support
- Advanced analytics dashboard
- Mobile app

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- OpenAI for GPT models and embeddings
- LangChain for RAG framework
- FastAPI for the excellent web framework
- React team for the frontend framework
- spaCy for NLP capabilities

---

**Built with â¤ï¸ for modern recruitment workflows**

**Current Version**: 1.0.0 (Tasks 14-15 Complete)  
**Last Updated**: October 31, 2025  
**Status**: ğŸŸ¢ Development Active
